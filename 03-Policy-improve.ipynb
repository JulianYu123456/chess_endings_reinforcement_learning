{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deterministic_policy(states):\n",
    "    pi = {}\n",
    "    for state, value in states.items():\n",
    "        pi[state] = list(value.keys())[0]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deterministic_policy_uniform(states):\n",
    "    pi = {}\n",
    "    for state, value in states.items():\n",
    "        pi[state] = np.random.choice(list(value.keys()))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_step_win_loss(states_actions, V, pi):\n",
    "    # \n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_state']\n",
    "        reward = actions[action]['status']\n",
    "        V_updated = 0\n",
    "        if next_node in V:\n",
    "            V_updated = -(reward + V[next_node])\n",
    "        else:\n",
    "            V_updated = -reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve_win_loss(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = [] # list(actions.keys())\n",
    "        expected_rewards = [] #np.zeros(len(actions))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            actions_list.append(action)\n",
    "            next_state = data['next_state']\n",
    "            reward = data['status']\n",
    "            if next_state in V:\n",
    "                expected_rewards.append(-(reward + V[next_state]))\n",
    "            else:\n",
    "                expected_rewards.append(-reward)\n",
    "\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "        if state == '4k3/8/4K2R/8/8/8/8/8 w':\n",
    "            print(np.argmax(expected_rewards))\n",
    "            print(actions_list)\n",
    "            print(expected_rewards)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_step_shortest_path(states_actions, V, pi, winning_reward=1e3):\n",
    "    # Evaluation in place (in contrast with evaluation with 2 arrays).\n",
    "    # Needs less memory and converges too\n",
    "    # pi is a dict and pi[s] is the best action for that state. (The most probable action)\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_state']\n",
    "        reward = actions[action]['status']\n",
    "        V_updated = 0\n",
    "        if next_node in V:\n",
    "            V_updated = -(reward + V[next_node]) - np.sign(-V[next_node])\n",
    "        else:\n",
    "            V_updated = -reward * winning_reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy_eval_step, states_actions, pi, theta, verbose=0):\n",
    "    if verbose:\n",
    "        print('Iteration number: ', end=' ')\n",
    "    \n",
    "    V = {}\n",
    "    iters = 0\n",
    "    for state in states_actions:\n",
    "        V[state] = 0\n",
    "    delta = theta + 1\n",
    "    while theta<delta: \n",
    "        V, delta = policy_eval_step(states_actions, V, pi)\n",
    "        iters += 1\n",
    "        if verbose:\n",
    "            print(iters, end=' ')\n",
    "    print()\n",
    "    return V, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve_shortest_path(V, states_actions, winning_reward=1e3):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = [] # list(actions.keys())\n",
    "        expected_rewards = [] #np.zeros(len(actions))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            actions_list.append(action)\n",
    "            next_state = data['next_state']\n",
    "            reward = data['status']\n",
    "            if next_state in V:\n",
    "                expected_rewards.append(-(reward + V[next_state]) - np.sign(-V[next_state]))\n",
    "            else:\n",
    "                expected_rewards.append(-reward * winning_reward)\n",
    "\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "        if state == '4k3/8/4K2R/8/8/8/8/8 w':\n",
    "            print(np.argmax(expected_rewards))\n",
    "            print(actions_list)\n",
    "            print(expected_rewards)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(states_actions, pi_old, deterministic_policy_eval_step = deterministic_policy_eval_step_shortest_path, policy_improve=policy_improve_shortest_path, verbose = 0):\n",
    "    states = shelve.open(filename, flag='r', writeback=True)\n",
    "    # Politica inicial\n",
    "    policy_updates = 100\n",
    "    while policy_updates > 0:\n",
    "        # Calculo values de politica\n",
    "        V, iters = policy_evaluation(deterministic_policy_eval_step, states_actions, pi_old, 1e-6, verbose=verbose)\n",
    "        # Mejoro pol√≠tica con values\n",
    "        pi = policy_improve(V, states_actions)\n",
    "\n",
    "        policy_updates = 0\n",
    "        for j, (state, accion) in enumerate(pi.items()):\n",
    "            if accion != pi_old[state]:\n",
    "                 policy_updates += 1\n",
    "        pi_old = pi.copy()\n",
    "        if verbose:\n",
    "            print('Number of differences of new policy vs old policy:', policy_updates)\n",
    "            print('---------------------------')\n",
    "    return pi_old, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/rook_states/states'\n",
    "states = shelve.open(filename, flag='r', writeback=True)\n",
    "initial_pi = get_deterministic_policy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, -0.0, -0.0, 0, 0, 0, 0, 0, 0, 998.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Number of differences of new policy vs old policy: 3940\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, -0.0, -0.0, 990.0, 0, 0, 0, 0, 0, 990.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Number of differences of new policy vs old policy: 64383\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, -0.0, 996.0, 0, 996.0, 996.0, 996.0, 996.0, 996.0, 0, -0.0, 0, 0, -0.0]\n",
      "Number of differences of new policy vs old policy: 151050\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, 982.0, 976.0, 978.0, 996.0, 996.0, 996.0, 996.0, 996.0, 970.0, 988.0, 980.0, 992.0, -0.0]\n",
      "Number of differences of new policy vs old policy: 212608\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, 984.0, 0, 988.0, 996.0, 996.0, 996.0, 996.0, 996.0, 0, 0, -0.0, 0, 0]\n",
      "Number of differences of new policy vs old policy: 211939\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "0\n",
      "['h6h8', 'h6h7', 'h6g6', 'h6f6', 'h6h5', 'h6h4', 'h6h3', 'h6h2', 'h6h1', 'e6f6', 'e6d6', 'e6f5', 'e6e5', 'e6d5']\n",
      "[1000.0, 0, -0.0, 0, 996.0, 996.0, 996.0, 996.0, 996.0, 0, 988.0, 0, -0.0, 988.0]\n",
      "Number of differences of new policy vs old policy: 171550\n",
      "---------------------------\n",
      "Iteration number:  1 "
     ]
    }
   ],
   "source": [
    "%time pi, V = policy_iteration(states, \\\n",
    "                 initial_pi, \\\n",
    "                 deterministic_policy_eval_step = deterministic_policy_eval_step_shortest_path, \\\n",
    "                 policy_improve=policy_improve_shortest_path, \\\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
